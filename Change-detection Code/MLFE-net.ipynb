{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":6968254,"sourceType":"datasetVersion","datasetId":4003580}],"dockerImageVersionId":30580,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport cv2\nimport numpy as np\n\n# Function to load and preprocess the images\ndef load_and_preprocess_images(directory, image_size=(128, 128)):\n    images = []\n    for filename in os.listdir(directory):\n        if(len(images)==2510):\n            break\n        if filename.endswith(\".jpg\") or filename.endswith(\".png\"):\n            image_path = os.path.join(directory, filename)\n            image = cv2.imread(image_path)\n            image = cv2.resize(image, image_size)  # Resize the image to the desired size\n            image = image.astype(np.float32) / 255.0  # Normalize pixel values to [0, 1]\n            images.append(image)\n    return np.array(images)\n\n# Load and preprocess the train and test images\ntrain_pre_images = load_and_preprocess_images(\"/kaggle/input/change-detection-sample/subset/train/A\")\ntrain_post_images = load_and_preprocess_images(\"/kaggle/input/change-detection-sample/subset/train/B\")\ntrain_change_maps = load_and_preprocess_images(\"/kaggle/input/change-detection-sample/subset/train/OUT\")\n\n# test_pre_images = load_and_preprocess_images(\"/kaggle/input/change-detection-sample/subset/test/A\")\n# test_post_images = load_and_preprocess_images(\"/kaggle/input/change-detection-sample/subset/test/B\")\n# test_change_maps = load_and_preprocess_images(\"/kaggle/input/change-detection-sample/subset/test/OUT\")\n\n# Make sure the number of images in train and test folders match\nassert len(train_pre_images) == len(train_post_images) == len(train_change_maps), \"Number of train images should be the same in all folders.\"\n# assert len(test_pre_images) == len(test_post_images) == len(test_change_maps), \"Number of test images should be the same in all folders.\"\n\n# Print the shape of the loaded images\nprint(\"Train Pre images shape:\", train_pre_images.shape)\nprint(\"Train Post images shape:\", train_post_images.shape)\nprint(\"Train Change maps shape:\", train_change_maps.shape)\n\n# print(\"Test Pre images shape:\", test_pre_images.shape)\n# print(\"Test Post images shape:\", test_post_images.shape)\n# print(\"Test Change maps shape:\", test_change_maps.shape)\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-11-16T11:20:59.192533Z","iopub.execute_input":"2023-11-16T11:20:59.193071Z","iopub.status.idle":"2023-11-16T11:22:30.773661Z","shell.execute_reply.started":"2023-11-16T11:20:59.193040Z","shell.execute_reply":"2023-11-16T11:22:30.772685Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def preprocessing(train_change_maps):\n    # Assuming you have a NumPy array 'label_images' containing your label images\n\n    # Apply the threshold to set all values greater than 0 to 1\n    thresholded_labels = np.where(train_change_maps > 0, 1, train_change_maps)\n\n    # 'thresholded_labels' now contains label images with all values greater than 0 as class 1\n    # Assuming you have a NumPy array 'binary_images' containing your binary images\n    # Each image is a 2D array with 0s and 1s\n\n    # Define the kernel for erosion and dilation (structuring element)\n    kernel = np.ones((3, 3), np.uint8)  # You can adjust the size as needed\n\n    # Create empty arrays to store preprocessed images\n    eroded_images = []\n    dilated_images = []\n\n    for image in thresholded_labels:\n        # Erosion\n        eroded_image = cv2.erode(image, kernel, iterations=1)\n        eroded_images.append(eroded_image)\n\n        # Dilation\n        dilated_image = cv2.dilate(eroded_image, kernel, iterations=1)\n        dilated_images.append(dilated_image)\n\n    # Now 'eroded_images' contains the images after erosion, and 'dilated_images' contains the images after dilation\n\n        # Convert the list of dilated images to a NumPy array\n    dilated_images_array = np.array(dilated_images)\n        \n    return dilated_images_array\n    \n    \n\n    \n    \n    ","metadata":{"execution":{"iopub.status.busy":"2023-11-16T11:22:30.775672Z","iopub.execute_input":"2023-11-16T11:22:30.776059Z","iopub.status.idle":"2023-11-16T11:22:30.783877Z","shell.execute_reply.started":"2023-11-16T11:22:30.776022Z","shell.execute_reply":"2023-11-16T11:22:30.782893Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dilated_images_array = preprocessing(train_change_maps)","metadata":{"execution":{"iopub.status.busy":"2023-11-16T11:22:30.785003Z","iopub.execute_input":"2023-11-16T11:22:30.785430Z","iopub.status.idle":"2023-11-16T11:22:36.040910Z","shell.execute_reply.started":"2023-11-16T11:22:30.785383Z","shell.execute_reply":"2023-11-16T11:22:36.039918Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nplt.figure(figsize=(15, 9))\nplt.subplot(4, 1, 1)\nplt.imshow(train_change_maps[119, :, :, 0],cmap='gray')\nplt.title(\"Before Preprocessing\")\nplt.subplot(4, 1, 2)\nplt.imshow(dilated_images_array[119, :, :, 0],cmap='gray')\nplt.title(\"After Preprocessing\")","metadata":{"execution":{"iopub.status.busy":"2023-11-16T11:22:36.043054Z","iopub.execute_input":"2023-11-16T11:22:36.043354Z","iopub.status.idle":"2023-11-16T11:22:36.505974Z","shell.execute_reply.started":"2023-11-16T11:22:36.043327Z","shell.execute_reply":"2023-11-16T11:22:36.505005Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from tensorflow.keras.preprocessing.image import ImageDataGenerator\n\n# # Create an ImageDataGenerator for data augmentation\n# datagen = ImageDataGenerator(\n#     rotation_range=50,\n#     width_shift_range=0.1,\n#     height_shift_range=0.1,\n#     shear_range=0.2,\n#     zoom_range=0.2,\n#     horizontal_flip=True,\n#     vertical_flip = True,\n#     fill_mode='nearest'\n# )","metadata":{"execution":{"iopub.status.busy":"2023-11-16T11:22:36.507147Z","iopub.execute_input":"2023-11-16T11:22:36.507432Z","iopub.status.idle":"2023-11-16T11:22:36.512056Z","shell.execute_reply.started":"2023-11-16T11:22:36.507406Z","shell.execute_reply":"2023-11-16T11:22:36.511117Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\n# Split the data for train_pre, train_post, and labels\nx_train_pre, x_valid_pre, x_train_post, x_valid_post, y_train, y_valid = train_test_split(train_pre_images, train_post_images, dilated_images_array, test_size=0.2, shuffle=True, random_state=42)\n\n# You now have:\n# - x_train_pre and y_train for training pre features and labels\n# - x_valid_pre and y_valid for validation pre features and labels\n# - x_train_post for training post features\n# - x_valid_post for validation post features\n","metadata":{"execution":{"iopub.status.busy":"2023-11-16T11:22:36.513165Z","iopub.execute_input":"2023-11-16T11:22:36.513449Z","iopub.status.idle":"2023-11-16T11:22:38.641107Z","shell.execute_reply.started":"2023-11-16T11:22:36.513420Z","shell.execute_reply":"2023-11-16T11:22:38.640307Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(x_train_pre.shape,y_train.shape,x_valid_pre.shape,y_valid.shape)","metadata":{"execution":{"iopub.status.busy":"2023-11-16T11:22:38.642313Z","iopub.execute_input":"2023-11-16T11:22:38.642833Z","iopub.status.idle":"2023-11-16T11:22:38.647897Z","shell.execute_reply.started":"2023-11-16T11:22:38.642790Z","shell.execute_reply":"2023-11-16T11:22:38.646973Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del train_pre_images\ndel train_post_images\ndel train_change_maps\ndel dilated_images_array","metadata":{"execution":{"iopub.status.busy":"2023-11-16T11:22:38.649406Z","iopub.execute_input":"2023-11-16T11:22:38.649769Z","iopub.status.idle":"2023-11-16T11:22:38.675072Z","shell.execute_reply.started":"2023-11-16T11:22:38.649735Z","shell.execute_reply":"2023-11-16T11:22:38.674034Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras import backend as K\n\n# recall\ndef recall_m(y_true, y_pred):\n    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n    recall = true_positives / (possible_positives + K.epsilon())\n    return recall\n\n# precision\ndef precision_m(y_true, y_pred):\n    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n    precision = true_positives / (predicted_positives + K.epsilon())\n    return precision\n\n#f1 score\ndef f1_m(y_true, y_pred):\n    precision = precision_m(y_true, y_pred)\n    recall = recall_m(y_true, y_pred)\n    return 2*((precision*recall)/(precision+recall+K.epsilon()))","metadata":{"execution":{"iopub.status.busy":"2023-11-16T11:22:38.676366Z","iopub.execute_input":"2023-11-16T11:22:38.676719Z","iopub.status.idle":"2023-11-16T11:22:56.557108Z","shell.execute_reply.started":"2023-11-16T11:22:38.676692Z","shell.execute_reply":"2023-11-16T11:22:56.556171Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nimport tensorflow as tf\nfrom tensorflow.keras import layers, models\nfrom tensorflow.keras.layers import Input, Conv2D, BatchNormalization, ReLU, Add, MaxPooling2D, GlobalAveragePooling2D, Dense\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras import layers, regularizers\nfrom tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, Concatenate, BatchNormalization, Activation, Dropout\n\ndef residual_block(x, filters, stride=1):\n    shortcut = x\n    x = Conv2D(filters, kernel_size=(3, 3), strides=stride, padding='same')(x)\n    x = BatchNormalization()(x)\n    x = ReLU()(x)\n    x = Conv2D(filters, kernel_size=(3, 3), strides=1,padding='same')(x)\n    x = BatchNormalization()(x)\n\n    if stride != 1 or shortcut.shape[-1] != filters:\n        shortcut = Conv2D(filters, kernel_size=(1, 1), strides=stride, padding='same')(shortcut)\n        shortcut = BatchNormalization()(shortcut)\n\n    x = Add()([x, shortcut])\n    x = ReLU()(x)\n    return x\n\ndef build_resnet18(input_shape):\n    input_tensor = Input(shape=input_shape)\n    \n    x = Conv2D(64, kernel_size=(3, 3), strides=2, padding='same')(input_tensor)\n    x = BatchNormalization()(x)\n    x = ReLU()(x)\n    x = MaxPooling2D(pool_size=(3, 3), strides=2, padding='same')(x)\n    \n    x = residual_block(x, 64)\n    x = residual_block(x, 64)\n    feature_map1 = x\n    x = residual_block(x, 128, 2)\n    x = residual_block(x, 128)\n    feature_map2 = x\n    x = residual_block(x, 256, 2)\n    x = residual_block(x, 256)\n    feature_map3 = x\n    x = residual_block(x, 512, 2)\n    x = residual_block(x, 512)\n    feature_map4 = x\n\n\n    model = Model(inputs=input_tensor, outputs=[feature_map1, feature_map2, feature_map3, feature_map4], name='resnet18')\n\n    return model\n","metadata":{"execution":{"iopub.status.busy":"2023-11-16T11:22:56.560675Z","iopub.execute_input":"2023-11-16T11:22:56.561195Z","iopub.status.idle":"2023-11-16T11:22:56.575687Z","shell.execute_reply.started":"2023-11-16T11:22:56.561167Z","shell.execute_reply":"2023-11-16T11:22:56.574660Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.layers import Input, Conv2D, BatchNormalization, Activation, Add,UpSampling2D\n\n# Define a convolutional block with 3x3 Conv, BN, and ReLU\ndef conv_block(x, filters):\n    x = Conv2D(filters, (3, 3), padding='same')(x)\n    x = BatchNormalization()(x)\n    x = Activation('relu')(x)\n    return x\n\n# Define a residual block with two consecutive convolutional layers\ndef residual_block_LFFM(x, filters):\n    shortcut = x  # Store the shortcut connection\n\n    # First convolutional layer\n    x1 = conv_block(x, filters)\n\n    # Second convolutional layer\n    x2 = Conv2D(filters, (3, 3), padding='same')(x1)\n    x2 = BatchNormalization()(x2)\n\n    # Apply ReLU activation\n    #x = Activation('relu')(x)\n    \n    return x2\n\n# Create a Keras model\ndef LFFM(height, width, num_channels,concatenate):\n\n    # Define constants\n    num_channels = num_channels  # Number of channels (Ci)\n    num_residual_blocks = 2  # Number of residual blocks in the first stage\n    Fcat = concatenate\n\n    Feature1 = residual_block_LFFM(Fcat, num_channels)\n    Fcat_conv = Conv2D(num_channels, (3, 3), padding='same')(Fcat)\n\n    # Apply 3x3 Conv and BN operations on Feature1 and Fcat\n    # Element-wise summation operation to obtain Feature2\n    Feature2 = Activation('relu')(tf.add(Feature1, Fcat_conv))\n\n    # Change the number of channels to 96\n    num_channels_2 = 96\n\n    # Apply the first convolution layer to obtain Feature3\n    Feature3 = residual_block_LFFM(Feature2, num_channels_2)\n    \n    Feature2_conv = Conv2D(num_channels_2, (3, 3), padding='same')(Feature2)\n    \n    #print(Feature3.shape,Feature2_conv.shape)\n\n    # Apply the second convolution layer to obtain Feature4\n    Feature4 = Activation('relu')(tf.add(Feature3,Feature2_conv))\n    \n    #print(Feature4.shape)\n    \n    return Feature4\n\n# You have Feature4 ready for further processing\n","metadata":{"execution":{"iopub.status.busy":"2023-11-16T11:22:56.576881Z","iopub.execute_input":"2023-11-16T11:22:56.577190Z","iopub.status.idle":"2023-11-16T11:22:56.610987Z","shell.execute_reply.started":"2023-11-16T11:22:56.577162Z","shell.execute_reply":"2023-11-16T11:22:56.610157Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def MSFA(F1):\n    global_avg_pooling_layer = tf.keras.layers.GlobalAveragePooling2D()\n    F1_gap = global_avg_pooling_layer(F1)\n    F1_gap = tf.expand_dims(tf.expand_dims(F1_gap, 1), 1)\n    F1_conv = tf.keras.layers.Conv2D(filters=384, kernel_size=(1, 1))(F1_gap)\n    #print(F1.shape)\n    F1_conv_relu = tf.keras.layers.ReLU()(F1_conv)\n    F2 = tf.keras.layers.Conv2D(filters=384, kernel_size=(1, 1))(F1_conv_relu)\n    #print(F2.shape)\n    concatenated_F1 = tf.concat([F1], axis=-1)\n    F3 = F2 * concatenated_F1\n    return F3\n","metadata":{"execution":{"iopub.status.busy":"2023-11-16T11:22:56.612139Z","iopub.execute_input":"2023-11-16T11:22:56.612492Z","iopub.status.idle":"2023-11-16T11:22:56.627243Z","shell.execute_reply.started":"2023-11-16T11:22:56.612449Z","shell.execute_reply":"2023-11-16T11:22:56.626472Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def classifier(input_shape,F3):\n    c1 = tf.keras.layers.Conv2D(384, (3, 3), padding='same', input_shape=input_shape)(F3)\n    c1 = tf.keras.layers.BatchNormalization()(c1)\n    c1 = tf.keras.layers.ReLU()(c1)\n    c1 = tf.keras.layers.Dropout(0.5)(c1)\n    \n    c2 = tf.keras.layers.Conv2D(384, (1, 1), padding='same')(c1)\n    c2 = tf.keras.layers.BatchNormalization()(c2)\n    c2 = tf.keras.layers.ReLU()(c2)\n    \n    up = tf.keras.layers.UpSampling2D(size=(2, 2))(c2)\n    up2 = tf.keras.layers.UpSampling2D(size=(2, 2))(up)\n    \n    result = tf.keras.layers.Conv2D(3, (3, 3), activation='sigmoid', padding='same')(up2)\n    return result\n    \n    \n    ","metadata":{"execution":{"iopub.status.busy":"2023-11-16T11:22:56.628460Z","iopub.execute_input":"2023-11-16T11:22:56.629156Z","iopub.status.idle":"2023-11-16T11:22:56.644078Z","shell.execute_reply.started":"2023-11-16T11:22:56.629121Z","shell.execute_reply":"2023-11-16T11:22:56.643302Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def siamese_autoencoder(input_shape):\n    pre_input = Input(shape=input_shape, name='pre_input')\n    post_input = Input(shape=input_shape, name='post_input')\n\n    encoder = build_resnet18(input_shape)\n\n    pre_encoded = encoder(pre_input)\n    post_encoded = encoder(post_input)\n    diff_encoded = [tf.abs(tf.subtract(pre_enc, post_enc)) for pre_enc, post_enc in zip(pre_encoded, post_encoded)]\n    \n    f_concat_1 = [tf.concat([pre_enc, post_enc], axis=-1) for pre_enc, post_enc in zip(pre_encoded, post_encoded)]\n    \n    f_concat_final = [tf.concat([pre_enc, post_enc], axis=-1) for pre_enc, post_enc in zip(f_concat_1, diff_encoded)]\n    \n    #print(len(f_concat_final))\n    LFFM_maps = []\n    for i in f_concat_final:\n        f = LFFM(i.shape[1], i.shape[2], i.shape[2],i)\n        #print(f.shape)\n        LFFM_maps.append(f)\n    #print(len(LFFM_maps))\n    \n    up1 = UpSampling2D(size=(2, 2))(LFFM_maps[-1])\n    concat1 = tf.concat([LFFM_maps[-2],up1],axis=-1)\n    up2 = UpSampling2D(size=(2, 2))(concat1)\n    concat2 = tf.concat([LFFM_maps[-3],up2],axis=-1)\n    up3 = UpSampling2D(size=(2, 2))(concat2)\n    final_concat = tf.concat([LFFM_maps[-4],up3],axis=-1)\n    #print(final_concat.shape)\n    \n    F3 = MSFA(final_concat)\n    print(F3.shape)\n    result = classifier((32,32,384),F3)\n    print(result.shape)\n    \n    return Model(inputs=[pre_input, post_input], outputs=result)\n        \n\n# Example usage:\ninput_shape = (256, 256, 3)\nmodel = siamese_autoencoder(input_shape)\n#model.summary()","metadata":{"execution":{"iopub.status.busy":"2023-11-16T11:22:56.645299Z","iopub.execute_input":"2023-11-16T11:22:56.645658Z","iopub.status.idle":"2023-11-16T11:23:03.829893Z","shell.execute_reply.started":"2023-11-16T11:22:56.645630Z","shell.execute_reply":"2023-11-16T11:23:03.828919Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nreduce_lr = tf.keras.callbacks.ReduceLROnPlateau(\n    monitor = 'val_accuracy',\n    patience = 50,\n    verbose = 1\n)\nearly_stop = tf.keras.callbacks.EarlyStopping(\n    monitor = 'val_accuracy',\n    patience = 50,\n    verbose = 1,\n    restore_best_weights = True\n)\nchkp = tf.keras.callbacks.ModelCheckpoint(\n    'EDM_checkpoint.h5',\n    monitor='val_accuracy',\n    verbose=1,\n    save_best_only=True\n)","metadata":{"execution":{"iopub.status.busy":"2023-11-16T11:23:03.831438Z","iopub.execute_input":"2023-11-16T11:23:03.832152Z","iopub.status.idle":"2023-11-16T11:23:03.839107Z","shell.execute_reply.started":"2023-11-16T11:23:03.832109Z","shell.execute_reply":"2023-11-16T11:23:03.838007Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras import layers, Model\nimport numpy as np\n\nfrom sklearn.utils import shuffle\n\n\nimport tensorflow as tf\n\n\ndef batch_balanced_contrastive_loss(y_true, y_pred, margin=2.2,lambda_=0.7):\n    # Extract D* and M* from y_pred\n    D_star = y_pred\n    M_star = y_true\n    \n    nu = tf.reduce_sum(1 - M_star)\n    nc = tf.reduce_sum(M_star)\n\n    # Calculate the contrastive loss for positive pairs (first term)\n    contrastive_loss = lambda_ * (1 / nu) * tf.reduce_sum(\n        (1 - M_star) *(D_star)\n    )\n\n    # Calculate the margin loss for negative pairs (second term)\n    negative_loss = (1 - lambda_) * (1 / nc) * tf.reduce_sum(\n        M_star * tf.maximum(0.0, margin - D_star)\n    )\n\n    # Total loss is the sum of the contrastive and negative losses\n    total_loss = contrastive_loss + negative_loss\n\n    return total_loss\n","metadata":{"execution":{"iopub.status.busy":"2023-11-16T11:23:03.840358Z","iopub.execute_input":"2023-11-16T11:23:03.840717Z","iopub.status.idle":"2023-11-16T11:23:03.851838Z","shell.execute_reply.started":"2023-11-16T11:23:03.840684Z","shell.execute_reply":"2023-11-16T11:23:03.851024Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def lr_schedule(epoch):\n    if epoch < 100:\n        return 0.001\n    else:\n        return 0.001 - (0.001 / 100) * (epoch - 100)\n\n\n\nopt = tf.keras.optimizers.Adam(learning_rate=0.001, beta_1=0.5, beta_2=0.999)\n\nmodel.compile(optimizer=opt, loss=batch_balanced_contrastive_loss, metrics=['accuracy', f1_m, precision_m, recall_m])\n\nlr_scheduler = tf.keras.callbacks.LearningRateScheduler(lr_schedule)\n\nhistory = model.fit([x_train_pre, x_train_post],\n          y_train,\n          batch_size=16,  \n          epochs=180,\n          validation_data=([x_valid_pre, x_valid_post],y_valid),\n          validation_batch_size=16,\n          callbacks=[chkp,lr_scheduler])","metadata":{"execution":{"iopub.status.busy":"2023-11-16T11:23:03.853080Z","iopub.execute_input":"2023-11-16T11:23:03.853509Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.save('final_model.h5')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nfig,((ax11,ax12),(ax13,ax14)) = plt.subplots(2,2,figsize=(20,15))\nax11.plot(history.history['loss'])\nax11.plot(history.history['val_loss'])\nax11.title.set_text('model loss')\nax11.set_ylabel('loss')\nax11.set_xlabel('epoch')\nax11.legend(['train', 'validation'], loc='upper left')\n\nax12.plot(history.history['precision_m'])\nax12.plot(history.history['val_precision_m'])\nax12.set_title('model precision')\nax12.set_ylabel('precision')\nax12.set_xlabel('epoch')\nax12.legend(['train', 'validation'], loc='upper left')\n\nax13.plot(history.history['recall_m'])\nax13.plot(history.history['val_recall_m'])\nax13.set_title('model recall')\nax13.set_ylabel('recall')\nax13.set_xlabel('epoch')\nax13.legend(['train', 'validation'], loc='upper left')\n\nax14.plot(history.history['f1_m'])\nax14.plot(history.history['val_f1_m'])\nax14.set_title('model f1')\nax14.set_ylabel('f1')\nax14.set_xlabel('epoch')\nax14.legend(['train', 'validation'], loc='upper left')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=[6,4])\nplt.plot(history.history['accuracy'], 'black', linewidth=2.0)\nplt.plot(history.history['val_accuracy'], 'blue', linewidth=2.0)\nplt.legend(['Training Accuracy', 'Validation Accuracy'], fontsize=14)\nplt.xlabel('Epochs', fontsize=10)\nplt.ylabel('Accuracy', fontsize=10)\nplt.title('Accuracy Curves', fontsize=12)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"loss, accuracy, f1_score, precision, recall = model.evaluate([x_valid_pre,x_valid_post],y_valid,batch_size=16, verbose=0)\nprint(loss, accuracy, f1_score, precision, recall)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"threshold = 0.25\npred_img = model.predict([x_valid_pre,x_valid_post],batch_size=16)\n#print(pred_img)\npred_img = (pred_img > threshold).astype(np.uint8)\n#print(pred_img[0])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nsample_index = 90\n# sample_pre_image = test_pre_images[sample_index]\n# sample_post_image = test_post_images[sample_index]\n\n# # Expand dimensions to match the model's input shape\n# sample_pre_image = np.expand_dims(sample_pre_image, axis=0)\n# sample_post_image = np.expand_dims(sample_post_image, axis=0)\n\n# # Use the trained model to predict the change map\n# predicted_change_map = model.predict([sample_pre_image, sample_post_image])\n\n#Plot the original pre and post images\nplt.figure(figsize=(15, 9))\nplt.subplot(4, 1, 1)\nplt.imshow(x_valid_pre[sample_index, :, :, 0:3])\nplt.title(\"Pre Image\")\nplt.subplot(4, 1, 2)\nplt.imshow(x_valid_post[sample_index, :, :, 0:3])\nplt.title(\"Post Image\")\n\nplt.subplot(4, 1, 3)\nplt.imshow(y_valid[sample_index, :, :, 0], cmap='gray')\nplt.title(\"Actual Change Map\")\n\nplt.subplot(4, 1, 4)\nplt.imshow(pred_img[sample_index, :, :, 0], cmap='gray')\nplt.title(\"Predicted Change Map\")\n\nplt.tight_layout()\nplt.show()\n\n# Calculate the total area (assuming each pixel represents a unit area)\ntotal_area = pred_img[sample_index, :, :, 0].size\n\n# Count the changed pixels (pixels with value 1)\nchanged_pixels = np.count_nonzero(pred_img[sample_index, :, :, 0] == 1)\n\n# Estimate the percentage of damage\npercentage_of_damage = (changed_pixels / total_area) * 100\n\nprint(f\"Percentage of Damage: {percentage_of_damage:.2f}%\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}