{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":6979146,"sourceType":"datasetVersion","datasetId":4010517},{"sourceId":7062305,"sourceType":"datasetVersion","datasetId":4065891}],"dockerImageVersionId":30588,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport cv2\nimport numpy as np\n\n# Function to load and preprocess the images\ndef load_and_preprocess_images(directory, image_size=(256, 256),crop_size=256):\n    images = []\n    for filename in os.listdir(directory):\n        if(len(images)==2500):\n            break\n        if filename.endswith(\".jpg\") or filename.endswith(\".png\"):\n            image_path = os.path.join(directory, filename)\n            image = cv2.imread(image_path)\n            #image = cv2.resize(image, image_size)  # Resize the image to the desired size\n            image = image.astype(np.float32) / 255.0  # Normalize pixel values to [0, 1]\n            images.append(image)\n    return np.array(images)\n\n# Load and preprocess the train and test images\ntrain_pre_images = load_and_preprocess_images(\"/kaggle/input/levir-cropped/Cropped/Train/A\")\ntrain_post_images = load_and_preprocess_images(\"/kaggle/input/levir-cropped/Cropped/Train/B\")\ntrain_change_maps = load_and_preprocess_images(\"/kaggle/input/levir-cropped/Cropped/Train/label\")\n\n# test_pre_images = load_and_preprocess_images(\"/kaggle/input/s2looking/LEVIR-CD+/LEVIR-CD+/test/A\")\n# test_post_images = load_and_preprocess_images(\"/kaggle/input/s2looking/LEVIR-CD+/LEVIR-CD+/test/B\")\n# test_change_maps = load_and_preprocess_images(\"/kaggle/input/s2looking/LEVIR-CD+/LEVIR-CD+/test/label\")\n\n# Make sure the number of images in train and test folders match\n#assert len(train_pre_images) == len(train_post_images) == len(train_change_maps), \"Number of train images should be the same in all folders.\"\n# assert len(test_pre_images) == len(test_post_images) == len(test_change_maps), \"Number of test images should be the same in all folders.\"\n\n# Print the shape of the loaded images\nprint(\"Train Pre images shape:\", train_pre_images.shape)\nprint(\"Train Post images shape:\", train_post_images.shape)\nprint(\"Train Change maps shape:\", train_change_maps.shape)\n\n# print(\"Test Pre images shape:\", test_pre_images.shape)\n# print(\"Test Post images shape:\", test_post_images.shape)\n# print(\"Test Change maps shape:\", test_change_maps.shape)\n\n\ndef preprocessing(train_change_maps):\n    # Assuming you have a NumPy array 'label_images' containing your label images\n\n    # Apply the threshold to set all values greater than 0 to 1\n    thresholded_labels = np.where(train_change_maps > 0, 1, train_change_maps)\n\n    # 'thresholded_labels' now contains label images with all values greater than 0 as class 1\n    # Assuming you have a NumPy array 'binary_images' containing your binary images\n    # Each image is a 2D array with 0s and 1s\n\n    # Define the kernel for erosion and dilation (structuring element)\n    kernel = np.ones((3, 3), np.uint8)  # You can adjust the size as needed\n\n    # Create empty arrays to store preprocessed images\n    eroded_images = []\n    dilated_images = []\n\n    for image in thresholded_labels:\n        # Erosion\n        eroded_image = cv2.erode(image, kernel, iterations=2)\n        eroded_images.append(eroded_image)\n\n        # Dilation\n        dilated_image = cv2.dilate(eroded_image, kernel, iterations=2)\n        dilated_images.append(dilated_image)\n\n    # Now 'eroded_images' contains the images after erosion, and 'dilated_images' contains the images after dilation\n\n        # Convert the list of dilated images to a NumPy array\n    dilated_images_array = np.array(dilated_images)\n        \n    return dilated_images_array\n    ","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-11-27T05:44:18.850392Z","iopub.execute_input":"2023-11-27T05:44:18.850672Z","iopub.status.idle":"2023-11-27T05:45:39.005216Z","shell.execute_reply.started":"2023-11-27T05:44:18.850648Z","shell.execute_reply":"2023-11-27T05:45:39.004205Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\n\ndilated_images_array = preprocessing(train_change_maps)\n","metadata":{"execution":{"iopub.status.busy":"2023-11-27T05:48:14.734051Z","iopub.execute_input":"2023-11-27T05:48:14.734930Z","iopub.status.idle":"2023-11-27T05:48:15.414135Z","shell.execute_reply.started":"2023-11-27T05:48:14.734893Z","shell.execute_reply":"2023-11-27T05:48:15.412632Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nplt.figure(figsize=(15, 9))\nplt.subplot(4, 1, 1)\nplt.imshow(train_pre_images[176, :, :, 0:3])\nplt.figure(figsize=(15, 9))\nplt.subplot(4, 1, 1)\nplt.imshow(train_post_images[176, :, :, 0:3])\nplt.figure(figsize=(15, 9))\nplt.subplot(4, 1, 1)\nplt.imshow(train_change_maps[176, :, :, 0],cmap='gray')\nplt.title(\"Before Preprocessing\")\nplt.subplot(4, 1, 2)\nplt.imshow(dilated_images_array[176, :, :, 0],cmap='gray')\nplt.title(\"After Preprocessing\")","metadata":{"execution":{"iopub.status.busy":"2023-11-27T05:48:25.651979Z","iopub.execute_input":"2023-11-27T05:48:25.652665Z","iopub.status.idle":"2023-11-27T05:48:26.378589Z","shell.execute_reply.started":"2023-11-27T05:48:25.652631Z","shell.execute_reply":"2023-11-27T05:48:26.377701Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from tensorflow.keras.preprocessing.image import ImageDataGenerator\n\n# # Create an ImageDataGenerator for data augmentation\n# datagen = ImageDataGenerator(\n#     rotation_range=50,\n#     width_shift_range=0.1,\n#     height_shift_range=0.1,\n#     shear_range=0.2,\n#     zoom_range=0.2,\n#     horizontal_flip=True,\n#     vertical_flip = True,\n#     fill_mode='nearest'\n# )","metadata":{"execution":{"iopub.status.busy":"2023-11-23T14:49:51.560220Z","iopub.execute_input":"2023-11-23T14:49:51.560543Z","iopub.status.idle":"2023-11-23T14:49:51.565268Z","shell.execute_reply.started":"2023-11-23T14:49:51.560514Z","shell.execute_reply":"2023-11-23T14:49:51.564285Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\n# Split the data for train_pre, train_post, and labels\nx_train_pre, x_valid_pre, x_train_post, x_valid_post, y_train, y_valid = train_test_split(train_pre_images, train_post_images, dilated_images_array, test_size=0.2, shuffle=True, random_state=42)\n\n# You now have:\n# - x_train_pre and y_train for training pre features and labels\n# - x_valid_pre and y_valid for validation pre features and labels\n# - x_train_post for training post features\n# - x_valid_post for validation post features\n","metadata":{"execution":{"iopub.status.busy":"2023-11-27T05:48:37.022783Z","iopub.execute_input":"2023-11-27T05:48:37.023384Z","iopub.status.idle":"2023-11-27T05:48:39.371094Z","shell.execute_reply.started":"2023-11-27T05:48:37.023347Z","shell.execute_reply":"2023-11-27T05:48:39.370017Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(x_train_pre.shape,y_train.shape,x_valid_pre.shape,y_valid.shape)","metadata":{"execution":{"iopub.status.busy":"2023-11-27T05:48:40.294997Z","iopub.execute_input":"2023-11-27T05:48:40.295671Z","iopub.status.idle":"2023-11-27T05:48:40.301395Z","shell.execute_reply.started":"2023-11-27T05:48:40.295632Z","shell.execute_reply":"2023-11-27T05:48:40.300409Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# del x_train_pre\n# del x_train_post\n# del y_train\n# del x_valid_pre\n# del x_valid_post\n# del y_valid","metadata":{"execution":{"iopub.status.busy":"2023-11-23T14:49:53.883631Z","iopub.execute_input":"2023-11-23T14:49:53.884656Z","iopub.status.idle":"2023-11-23T14:49:53.891830Z","shell.execute_reply.started":"2023-11-23T14:49:53.884615Z","shell.execute_reply":"2023-11-23T14:49:53.891119Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del train_pre_images\ndel train_post_images\ndel train_change_maps\ndel dilated_images_array","metadata":{"execution":{"iopub.status.busy":"2023-11-27T05:48:43.486471Z","iopub.execute_input":"2023-11-27T05:48:43.486798Z","iopub.status.idle":"2023-11-27T05:48:43.507279Z","shell.execute_reply.started":"2023-11-27T05:48:43.486771Z","shell.execute_reply":"2023-11-27T05:48:43.506360Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras import backend as K\n\n# recall\ndef recall_m(y_true, y_pred):\n    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n    recall = true_positives / (possible_positives + K.epsilon())\n    return recall\n\n# precision\ndef precision_m(y_true, y_pred):\n    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n    precision = true_positives / (predicted_positives + K.epsilon())\n    return precision\n\n#f1 score\ndef f1_m(y_true, y_pred):\n    precision = precision_m(y_true, y_pred)\n    recall = recall_m(y_true, y_pred)\n    return 2*((precision*recall)/(precision+recall+K.epsilon()))","metadata":{"execution":{"iopub.status.busy":"2023-11-27T05:48:46.092396Z","iopub.execute_input":"2023-11-27T05:48:46.092728Z","iopub.status.idle":"2023-11-27T05:49:04.418539Z","shell.execute_reply.started":"2023-11-27T05:48:46.092700Z","shell.execute_reply":"2023-11-27T05:49:04.417497Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def channel_attention_module(x, ratio=8):\n    batch, _, _, channel = x.shape\n\n    ## Shared layers\n    l1 = Dense(channel//ratio, activation=\"relu\", use_bias=False)\n    l2 = Dense(channel, use_bias=False)\n\n    ## Global Average Pooling\n    x1 = GlobalAveragePooling2D()(x)\n    x1 = l1(x1)\n    x1 = l2(x1)\n\n    ## Global Max Pooling\n    x2 = GlobalMaxPooling2D()(x)\n    x2 = l1(x2)\n    x2 = l2(x2)\n\n    ## Add both the features and pass through sigmoid\n    feats = x1 + x2\n    feats = Activation(\"sigmoid\")(feats)\n    feats = Multiply()([x, feats])\n\n    return feats","metadata":{"execution":{"iopub.status.busy":"2023-11-27T05:49:04.420510Z","iopub.execute_input":"2023-11-27T05:49:04.421218Z","iopub.status.idle":"2023-11-27T05:49:04.428767Z","shell.execute_reply.started":"2023-11-27T05:49:04.421180Z","shell.execute_reply":"2023-11-27T05:49:04.427611Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def spatial_attention_module(x):\n    ## Average Pooling\n    x1 = tf.reduce_mean(x, axis=-1)\n    x1 = tf.expand_dims(x1, axis=-1)\n\n    ## Max Pooling\n    x2 = tf.reduce_max(x, axis=-1)\n    x2 = tf.expand_dims(x2, axis=-1)\n\n    ## Concatenat both the features\n    feats = Concatenate()([x1, x2])\n    ## Conv layer\n    feats = Conv2D(1, kernel_size=7, padding=\"same\", activation=\"sigmoid\")(feats)\n    feats = Multiply()([x, feats])\n\n    return feats","metadata":{"execution":{"iopub.status.busy":"2023-11-27T05:49:04.430029Z","iopub.execute_input":"2023-11-27T05:49:04.430422Z","iopub.status.idle":"2023-11-27T05:49:04.463125Z","shell.execute_reply.started":"2023-11-27T05:49:04.430385Z","shell.execute_reply":"2023-11-27T05:49:04.462290Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def cbam(x):\n    x = channel_attention_module(x)\n    x = spatial_attention_module(x)\n    return x","metadata":{"execution":{"iopub.status.busy":"2023-11-27T05:49:04.466098Z","iopub.execute_input":"2023-11-27T05:49:04.466516Z","iopub.status.idle":"2023-11-27T05:49:04.482111Z","shell.execute_reply.started":"2023-11-27T05:49:04.466484Z","shell.execute_reply":"2023-11-27T05:49:04.481194Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nimport tensorflow as tf\nfrom tensorflow.keras import layers, models\nfrom tensorflow.keras.layers import Input, Conv2D, BatchNormalization, ReLU, Add, MaxPooling2D, GlobalAveragePooling2D, Dense\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras import layers, regularizers\nfrom tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, Concatenate, BatchNormalization, Activation, Dropout\n\ndef residual_block(x, filters, stride=1):\n    shortcut = x\n    x = Conv2D(filters, kernel_size=(3, 3), strides=stride, padding='same')(x)\n    x = BatchNormalization()(x)\n    x = ReLU()(x)\n    x = Conv2D(filters, kernel_size=(3, 3), strides=1,padding='same')(x)\n    x = BatchNormalization()(x)\n\n    if stride != 1 or shortcut.shape[-1] != filters:\n        shortcut = Conv2D(filters, kernel_size=(1, 1), strides=stride, padding='same')(shortcut)\n        shortcut = BatchNormalization()(shortcut)\n\n    x = Add()([x, shortcut])\n    x = ReLU()(x)\n    return x\n\ndef build_resnet18(input_shape):\n    input_tensor = Input(shape=input_shape)\n    \n    x = Conv2D(64, kernel_size=(3, 3), strides=2, padding='same')(input_tensor)\n    x = BatchNormalization()(x)\n    x = ReLU()(x)\n    x = MaxPooling2D(pool_size=(3, 3), strides=2, padding='same')(x)\n    \n    x = residual_block(x, 64)\n    x = residual_block(x, 64)\n    feature_map1 = x\n    x = residual_block(x, 128, 2)\n    x = residual_block(x, 128)\n    feature_map2 = x\n    x = residual_block(x, 256, 2)\n    x = residual_block(x, 256)\n    feature_map3 = x\n    x = residual_block(x, 512, 2)\n    x = residual_block(x, 512)\n    feature_map4 = cbam(x) #Added Attention Module\n\n\n    model = Model(inputs=input_tensor, outputs=[feature_map1, feature_map2, feature_map3, feature_map4], name='resnet18')\n\n    return model\n","metadata":{"execution":{"iopub.status.busy":"2023-11-27T05:49:04.484696Z","iopub.execute_input":"2023-11-27T05:49:04.485013Z","iopub.status.idle":"2023-11-27T05:49:04.500640Z","shell.execute_reply.started":"2023-11-27T05:49:04.484982Z","shell.execute_reply":"2023-11-27T05:49:04.499762Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.layers import Input, Conv2D, BatchNormalization, Activation, Add,UpSampling2D, GlobalMaxPooling2D\n\n# Define a convolutional block with 3x3 Conv, BN, and ReLU\ndef conv_block(x, filters):\n    x = Conv2D(filters, (3, 3), padding='same')(x)\n    x = BatchNormalization()(x)\n    x = Activation('relu')(x)\n    return x\n\n# Define a residual block with two consecutive convolutional layers\ndef residual_block_LFFM(x, filters):\n    shortcut = x  # Store the shortcut connection\n\n    # First convolutional layer\n    x1 = conv_block(x, filters)\n\n    # Second convolutional layer\n    x2 = Conv2D(filters, (3, 3), padding='same')(x1)\n    x2 = BatchNormalization()(x2)\n\n    # Apply ReLU activation\n    #x = Activation('relu')(x)\n    \n    return x2\n\n# Create a Keras model\ndef LFFM(height, width, num_channels,concatenate):\n\n    # Define constants\n    num_channels = num_channels  # Number of channels (Ci)\n    num_residual_blocks = 2  # Number of residual blocks in the first stage\n    Fcat = concatenate\n\n    Feature1 = residual_block_LFFM(Fcat, num_channels)\n    Fcat_conv = Conv2D(num_channels, (3, 3), padding='same')(Fcat)\n\n    # Apply 3x3 Conv and BN operations on Feature1 and Fcat\n    # Element-wise summation operation to obtain Feature2\n    Feature2 = Activation('relu')(tf.add(Feature1, Fcat_conv))\n\n    # Change the number of channels to 96\n    num_channels_2 = 96\n\n    # Apply the first convolution layer to obtain Feature3\n    Feature3 = residual_block_LFFM(Feature2, num_channels_2)\n    \n    Feature2_conv = Conv2D(num_channels_2, (3, 3), padding='same')(Feature2)\n    \n    #print(Feature3.shape,Feature2_conv.shape)\n\n    # Apply the second convolution layer to obtain Feature4\n    Feature4 = Activation('relu')(tf.add(Feature3,Feature2_conv))\n    \n    #print(Feature4.shape)\n    \n    return Feature4\n\n# You have Feature4 ready for further processing\n","metadata":{"execution":{"iopub.status.busy":"2023-11-27T05:49:04.502994Z","iopub.execute_input":"2023-11-27T05:49:04.503480Z","iopub.status.idle":"2023-11-27T05:49:04.519162Z","shell.execute_reply.started":"2023-11-27T05:49:04.503341Z","shell.execute_reply":"2023-11-27T05:49:04.518305Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def MSFA(F1):\n    global_avg_pooling_layer = tf.keras.layers.GlobalAveragePooling2D()\n    F1_gap = global_avg_pooling_layer(F1)\n    F1_gap = tf.expand_dims(tf.expand_dims(F1_gap, 1), 1)\n    F1_conv = tf.keras.layers.Conv2D(filters=384, kernel_size=(1, 1))(F1_gap)\n    #print(F1.shape)\n    F1_conv_relu = tf.keras.layers.ReLU()(F1_conv)\n    F2 = tf.keras.layers.Conv2D(filters=384, kernel_size=(1, 1))(F1_conv_relu)\n    #print(F2.shape)\n    concatenated_F1 = tf.concat([F1], axis=-1)\n    F3 = F2 * concatenated_F1\n    return F3","metadata":{"execution":{"iopub.status.busy":"2023-11-27T05:49:04.520359Z","iopub.execute_input":"2023-11-27T05:49:04.520680Z","iopub.status.idle":"2023-11-27T05:49:04.539142Z","shell.execute_reply.started":"2023-11-27T05:49:04.520650Z","shell.execute_reply":"2023-11-27T05:49:04.538325Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def classifier(input_shape,F3):\n    c1 = tf.keras.layers.Conv2D(384, (3, 3), padding='same', input_shape=input_shape)(F3)\n    c1 = tf.keras.layers.BatchNormalization()(c1)\n    c1 = tf.keras.layers.ReLU()(c1)\n    c1 = tf.keras.layers.Dropout(0.5)(c1)\n    \n    c2 = tf.keras.layers.Conv2D(384, (1, 1), padding='same')(c1)\n    c2 = tf.keras.layers.BatchNormalization()(c2)\n    c2 = tf.keras.layers.ReLU()(c2)\n    \n    up = tf.keras.layers.UpSampling2D(size=(2, 2))(c2)\n    up2 = tf.keras.layers.UpSampling2D(size=(2, 2))(up)\n    \n    result = tf.keras.layers.Conv2D(3, (3, 3), activation='sigmoid', padding='same')(up2)\n    return result","metadata":{"execution":{"iopub.status.busy":"2023-11-27T05:49:04.540498Z","iopub.execute_input":"2023-11-27T05:49:04.540748Z","iopub.status.idle":"2023-11-27T05:49:04.557633Z","shell.execute_reply.started":"2023-11-27T05:49:04.540725Z","shell.execute_reply":"2023-11-27T05:49:04.556714Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.layers import GlobalAveragePooling2D, GlobalMaxPooling2D, Reshape, Dense, Input\nfrom tensorflow.keras.layers import Activation, Concatenate, Conv2D, Multiply","metadata":{"execution":{"iopub.status.busy":"2023-11-27T05:49:04.558925Z","iopub.execute_input":"2023-11-27T05:49:04.559235Z","iopub.status.idle":"2023-11-27T05:49:04.568804Z","shell.execute_reply.started":"2023-11-27T05:49:04.559204Z","shell.execute_reply":"2023-11-27T05:49:04.567936Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def siamese_autoencoder(input_shape):\n    pre_input = Input(shape=input_shape, name='pre_input')\n    post_input = Input(shape=input_shape, name='post_input')\n\n    encoder = build_resnet18(input_shape)\n\n    pre_encoded = encoder(pre_input)\n    post_encoded = encoder(post_input)\n    diff_encoded = [tf.abs(tf.subtract(pre_enc, post_enc)) for pre_enc, post_enc in zip(pre_encoded, post_encoded)]\n    \n    f_concat_1 = [tf.concat([pre_enc, post_enc], axis=-1) for pre_enc, post_enc in zip(pre_encoded, post_encoded)]\n    \n    f_concat_final = [tf.concat([pre_enc, post_enc], axis=-1) for pre_enc, post_enc in zip(f_concat_1, diff_encoded)]\n    \n    #print(len(f_concat_final))\n    LFFM_maps = []\n    for i in f_concat_final:\n        f = LFFM(i.shape[1], i.shape[2], i.shape[2],i)\n        #print(f.shape)\n        LFFM_maps.append(f)\n    #print(len(LFFM_maps))\n    \n    up1 = UpSampling2D(size=(2, 2))(LFFM_maps[-1])\n    up1 = cbam(up1) #Added Attention Layer\n    concat1 = tf.concat([LFFM_maps[-2],up1],axis=-1)\n    \n    up2 = UpSampling2D(size=(2, 2))(concat1)\n    up2 = cbam(up2) #Added Attention Layer\n    concat2 = tf.concat([LFFM_maps[-3],up2],axis=-1)\n    \n    up3 = UpSampling2D(size=(2, 2))(concat2)\n    final_concat = tf.concat([LFFM_maps[-4],up3],axis=-1)\n    #print(final_concat.shape)\n    \n    F3 = MSFA(final_concat)\n    print(F3.shape)\n    result = classifier((32,32,384),F3)\n    print(result.shape)\n    \n    return Model(inputs=[pre_input, post_input], outputs=result)\n        \n\n# Example usage:\ninput_shape = (256, 256, 3)\nmodel = siamese_autoencoder(input_shape)\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2023-11-27T05:49:06.437282Z","iopub.execute_input":"2023-11-27T05:49:06.437649Z","iopub.status.idle":"2023-11-27T05:49:14.634292Z","shell.execute_reply.started":"2023-11-27T05:49:06.437617Z","shell.execute_reply":"2023-11-27T05:49:14.633337Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nreduce_lr = tf.keras.callbacks.ReduceLROnPlateau(\n    monitor = 'val_accuracy',\n    patience = 50,\n    verbose = 1\n)\nearly_stop = tf.keras.callbacks.EarlyStopping(\n    monitor = 'val_accuracy',\n    patience = 50,\n    verbose = 1,\n    restore_best_weights = True\n)\nchkp = tf.keras.callbacks.ModelCheckpoint(\n    'EDM_checkpoint.h5',\n    monitor='val_accuracy',\n    verbose=1,\n    save_best_only=True\n)","metadata":{"execution":{"iopub.status.busy":"2023-11-27T05:49:42.795997Z","iopub.execute_input":"2023-11-27T05:49:42.796384Z","iopub.status.idle":"2023-11-27T05:49:42.802143Z","shell.execute_reply.started":"2023-11-27T05:49:42.796349Z","shell.execute_reply":"2023-11-27T05:49:42.801316Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras import layers, Model\nimport numpy as np\n\nfrom sklearn.utils import shuffle\n\n\n# def dice_loss(y_true, y_pred):\n#   y_true = tf.cast(y_true, tf.float32)\n#   y_pred = tf.math.sigmoid(y_pred)\n#   numerator = 2 * tf.reduce_sum(y_true * y_pred)\n#   denominator = tf.reduce_sum(y_true + y_pred)\n#   return 1 - numerator / denominator\n\n\nimport tensorflow as tf\n\ndef batch_balanced_contrastive_loss(y_true, y_pred, margin=2.2,lambda_=0.7):\n    # Extract D* and M* from y_pred\n    D_star = y_pred\n    M_star = y_true\n    \n    nu = tf.reduce_sum(1 - M_star)\n    nc = tf.reduce_sum(M_star)\n\n    # Calculate the contrastive loss for positive pairs (first term)   #Handles false postives\n    contrastive_loss = lambda_ * (1 / nu + 1e-8) * tf.reduce_sum(\n        (1 - M_star) *(D_star)\n    )\n\n    # Calculate the margin loss for negative pairs (second term)   #Handles false negatives\n    negative_loss = (1 - lambda_) * (1 / nc + 1e-8) * tf.reduce_sum(\n        M_star * tf.maximum(0.0, margin - D_star)\n    )\n\n    # Total loss is the sum of the contrastive and negative losses\n    total_loss = contrastive_loss + negative_loss\n\n    return total_loss\n","metadata":{"execution":{"iopub.status.busy":"2023-11-27T05:49:45.399723Z","iopub.execute_input":"2023-11-27T05:49:45.400655Z","iopub.status.idle":"2023-11-27T05:49:45.408338Z","shell.execute_reply.started":"2023-11-27T05:49:45.400619Z","shell.execute_reply":"2023-11-27T05:49:45.407374Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def lr_schedule(epoch):\n    if epoch < 100:\n        return 0.001\n    else:\n        return 0.001 - (0.001 / 100) * (epoch - 100)\n\n\nopt = tf.keras.optimizers.Adam(learning_rate=0.001, beta_1=0.5, beta_2=0.999)\n\nmodel.compile(optimizer=opt, loss=batch_balanced_contrastive_loss, metrics=['accuracy', f1_m, precision_m, recall_m])\n\nlr_scheduler = tf.keras.callbacks.LearningRateScheduler(lr_schedule)\n\nhistory = model.fit([x_train_pre, x_train_post],\n          y_train,\n          batch_size=16,  \n          epochs=180,\n          validation_data=([x_valid_pre, x_valid_post],y_valid),\n          validation_batch_size = 16,\n          callbacks=[chkp,lr_scheduler])","metadata":{"execution":{"iopub.status.busy":"2023-11-27T05:49:57.906483Z","iopub.execute_input":"2023-11-27T05:49:57.906841Z","iopub.status.idle":"2023-11-27T09:48:45.444532Z","shell.execute_reply.started":"2023-11-27T05:49:57.906810Z","shell.execute_reply":"2023-11-27T09:48:45.443594Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.save('final_model.h5')","metadata":{"execution":{"iopub.status.busy":"2023-11-23T18:47:04.190773Z","iopub.execute_input":"2023-11-23T18:47:04.191089Z","iopub.status.idle":"2023-11-23T18:47:04.930306Z","shell.execute_reply.started":"2023-11-23T18:47:04.191064Z","shell.execute_reply":"2023-11-23T18:47:04.929209Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Load the model","metadata":{}},{"cell_type":"code","source":"model = tf.keras.models.load_model('/kaggle/input/modelwithweight/final_model.h5')","metadata":{"execution":{"iopub.status.busy":"2023-11-23T18:47:04.931696Z","iopub.execute_input":"2023-11-23T18:47:04.931965Z","iopub.status.idle":"2023-11-23T18:47:06.382076Z","shell.execute_reply.started":"2023-11-23T18:47:04.931943Z","shell.execute_reply":"2023-11-23T18:47:06.380722Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nfig,((ax11,ax12),(ax13,ax14)) = plt.subplots(2,2,figsize=(20,15))\nax11.plot(history.history['loss'])\nax11.plot(history.history['val_loss'])\nax11.title.set_text('model loss')\nax11.set_ylabel('loss')\nax11.set_xlabel('epoch')\nax11.legend(['train', 'validation'], loc='upper left')\n\nax12.plot(history.history['precision_m'])\nax12.plot(history.history['val_precision_m'])\nax12.set_title('model precision')\nax12.set_ylabel('precision')\nax12.set_xlabel('epoch')\nax12.legend(['train', 'validation'], loc='upper left')\n\nax13.plot(history.history['recall_m'])\nax13.plot(history.history['val_recall_m'])\nax13.set_title('model recall')\nax13.set_ylabel('recall')\nax13.set_xlabel('epoch')\nax13.legend(['train', 'validation'], loc='upper left')\n\nax14.plot(history.history['f1_m'])\nax14.plot(history.history['val_f1_m'])\nax14.set_title('model f1')\nax14.set_ylabel('f1')\nax14.set_xlabel('epoch')\nax14.legend(['train', 'validation'], loc='upper left')","metadata":{"execution":{"iopub.status.busy":"2023-11-27T10:02:55.736659Z","iopub.execute_input":"2023-11-27T10:02:55.737673Z","iopub.status.idle":"2023-11-27T10:02:56.909188Z","shell.execute_reply.started":"2023-11-27T10:02:55.737626Z","shell.execute_reply":"2023-11-27T10:02:56.908209Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=[6,4])\nplt.plot(history.history['accuracy'], 'black', linewidth=2.0)\nplt.plot(history.history['val_accuracy'], 'blue', linewidth=2.0)\nplt.legend(['Training Accuracy', 'Validation Accuracy'], fontsize=14)\nplt.xlabel('Epochs', fontsize=10)\nplt.ylabel('Accuracy', fontsize=10)\nplt.title('Accuracy Curves', fontsize=12)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"loss, accuracy, f1_score, precision, recall = model.evaluate([x_valid_pre,x_valid_post],y_valid,batch_size=16, verbose=0)\nprint(loss, accuracy, f1_score, precision, recall)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"threshold = 0.25\npred_img = model.predict([x_valid_pre,x_valid_post],batch_size=16)\n#print(pred_img)\n#pred_img = (pred_img > threshold).astype(np.uint8)\n#print(pred_img[0])","metadata":{"execution":{"iopub.status.busy":"2023-11-23T18:47:06.388367Z","iopub.status.idle":"2023-11-23T18:47:06.388690Z","shell.execute_reply.started":"2023-11-23T18:47:06.388533Z","shell.execute_reply":"2023-11-23T18:47:06.388548Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nsample_index = 7\n# sample_pre_image = test_pre_images[sample_index]\n# sample_post_image = test_post_images[sample_index]\n\n# # Expand dimensions to match the model's input shape\n# sample_pre_image = np.expand_dims(sample_pre_image, axis=0)\n# sample_post_image = np.expand_dims(sample_post_image, axis=0)\n\n# # Use the trained model to predict the change map\n# predicted_change_map = model.predict([sample_pre_image, sample_post_image])\n\n#Plot the original pre and post images\nplt.figure(figsize=(15, 9))\nplt.subplot(4, 1, 1)\nplt.imshow(x_valid_pre[sample_index, :, :, 0:3])\nplt.title(\"Pre Image\")\nplt.subplot(4, 1, 2)\nplt.imshow(x_valid_post[sample_index, :, :, 0:3])\nplt.title(\"Post Image\")\n\nplt.subplot(4, 1, 3)\nplt.imshow(y_valid[sample_index, :, :, 0], cmap='gray')\nplt.title(\"Actual Change Map\")\n\nplt.subplot(4, 1, 4)\nplt.imshow(pred_img[sample_index, :, :, 0], cmap='gray')\nplt.title(\"Predicted Change Map\")\n\nplt.tight_layout()\nplt.show()\n\n# Calculate the total area (assuming each pixel represents a unit area)\ntotal_area = pred_img[sample_index, :, :, 0].size\n\n# Count the changed pixels (pixels with value 1)\nchanged_pixels = np.count_nonzero(pred_img[sample_index, :, :, 0] == 1)\n\n# Estimate the percentage of damage\npercentage_of_damage = (changed_pixels / total_area) * 100\n\nprint(f\"Percentage of Damage: {percentage_of_damage:.2f}%\")","metadata":{"execution":{"iopub.status.busy":"2023-11-23T18:47:06.389840Z","iopub.status.idle":"2023-11-23T18:47:06.390212Z","shell.execute_reply.started":"2023-11-23T18:47:06.390033Z","shell.execute_reply":"2023-11-23T18:47:06.390051Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Testing","metadata":{}},{"cell_type":"code","source":"# test_pre_images = load_and_preprocess_images(\"/kaggle/input/s2looking/LEVIR-CD+/LEVIR-CD+/test/A\")\n# test_post_images = load_and_preprocess_images(\"/kaggle/input/s2looking/LEVIR-CD+/LEVIR-CD+/test/B\")\n# test_change_maps = load_and_preprocess_images(\"/kaggle/input/s2looking/LEVIR-CD+/LEVIR-CD+/test/label\")","metadata":{"execution":{"iopub.status.busy":"2023-11-23T18:47:06.391592Z","iopub.status.idle":"2023-11-23T18:47:06.391922Z","shell.execute_reply.started":"2023-11-23T18:47:06.391758Z","shell.execute_reply":"2023-11-23T18:47:06.391775Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# print(\"Test Pre images shape:\", test_pre_images.shape)\n# print(\"Test Post images shape:\", test_post_images.shape)\n# print(\"Test Change maps shape:\", test_change_maps.shape)","metadata":{"execution":{"iopub.status.busy":"2023-11-23T18:47:06.393218Z","iopub.status.idle":"2023-11-23T18:47:06.393527Z","shell.execute_reply.started":"2023-11-23T18:47:06.393372Z","shell.execute_reply":"2023-11-23T18:47:06.393386Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# loss, accuracy, f1_score, precision, recall = model.evaluate([test_pre_images,test_post_images],test_change_maps, verbose=0)\n# print(loss, accuracy, f1_score, precision, recall)","metadata":{"execution":{"iopub.status.busy":"2023-11-23T18:47:06.394420Z","iopub.status.idle":"2023-11-23T18:47:06.394773Z","shell.execute_reply.started":"2023-11-23T18:47:06.394598Z","shell.execute_reply":"2023-11-23T18:47:06.394615Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# pred_img_test = model.predict([test_pre_images,test_post_images],batch_size=16)","metadata":{"execution":{"iopub.status.busy":"2023-11-23T18:47:06.396322Z","iopub.status.idle":"2023-11-23T18:47:06.396774Z","shell.execute_reply.started":"2023-11-23T18:47:06.396542Z","shell.execute_reply":"2023-11-23T18:47:06.396564Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import matplotlib.pyplot as plt\n# sample_index = 16\n# # sample_pre_image = test_pre_images[sample_index]\n# # sample_post_image = test_post_images[sample_index]\n\n# # # Expand dimensions to match the model's input shape\n# # sample_pre_image = np.expand_dims(sample_pre_image, axis=0)\n# # sample_post_image = np.expand_dims(sample_post_image, axis=0)\n\n# # # Use the trained model to predict the change map\n# # predicted_change_map = model.predict([sample_pre_image, sample_post_image])\n\n# #Plot the original pre and post images\n# plt.figure(figsize=(15, 9))\n# plt.subplot(4, 1, 1)\n# plt.imshow(test_pre_images[sample_index, :, :, 0:3])\n# plt.title(\"Pre Image\")\n# plt.subplot(4, 1, 2)\n# plt.imshow(test_post_images[sample_index, :, :, 0:3])\n# plt.title(\"Post Image\")\n\n# plt.subplot(4, 1, 3)\n# plt.imshow(test_change_maps[sample_index, :, :, 0], cmap='gray')\n# plt.title(\"Actual Change Map\")\n\n# plt.subplot(4, 1, 4)\n# plt.imshow(pred_img_test[sample_index, :, :, 0], cmap='gray')\n# plt.title(\"Predicted Change Map\")\n\n# plt.tight_layout()\n# plt.show()\n\n# # Calculate the total area (assuming each pixel represents a unit area)\n# total_area = pred_img_test[sample_index, :, :, 0].size\n\n# # Count the changed pixels (pixels with value 1)\n# changed_pixels = np.count_nonzero(pred_img_test[sample_index, :, :, 0] == 1)\n\n# # Estimate the percentage of damage\n# percentage_of_damage = (changed_pixels / total_area) * 100\n\n# print(f\"Percentage of Damage: {percentage_of_damage:.2f}%\")","metadata":{"execution":{"iopub.status.busy":"2023-11-23T18:47:06.398413Z","iopub.status.idle":"2023-11-23T18:47:06.398858Z","shell.execute_reply.started":"2023-11-23T18:47:06.398629Z","shell.execute_reply":"2023-11-23T18:47:06.398650Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}